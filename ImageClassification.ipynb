{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ff87dad-ba74-472e-acdb-da3c86322a4d",
   "metadata": {},
   "source": [
    "## Load and Customize a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e9641ad-e40c-40e2-b84a-3d432a0894af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading pretrained model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# Loads the pretrained version of the resnet CNN\n",
    "weights = models.ResNet18_Weights.DEFAULT\n",
    "model = models.resnet18(weights=weights)\n",
    "\n",
    "# Customize the final layer to match the number of classes in your dataset\n",
    "num_ftrs = model.fc.in_features # retrieves the features in the final fully connected (fc) layer \n",
    "                                # to define the new final layer correctly\n",
    "model.fc = nn.Linear(num_ftrs, 4)  # output probabilities for 4 categories\n",
    "\n",
    "# Move the model to the device (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Done loading pretrained model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9757876f-2da0-4b1b-9333-42c960fff381",
   "metadata": {},
   "source": [
    "## Define Dataset and DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32e87c19-87cf-472d-9a73-b2336142ee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader \n",
    "                            # represents data in a PyTorch compatible format\n",
    "                            # efficiently loads batches of custom data\n",
    "from PIL import Image # \"Pillow\" or Python Imaging Library\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.annotations.iloc[idx, 0]) # retrieves value at specialized index and column 0\n",
    "        image = Image.open(img_path).convert(\"RGB\") # easy to convert to tensors\n",
    "\n",
    "        # Crop the right half of the image\n",
    "        width, height = image.size\n",
    "        image = image.crop((width // 2, 0, width, height))\n",
    "                        # right half, starts from top of image\n",
    "        category = self.annotations.iloc[idx, 1]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # creating a dictionary called sample\n",
    "        sample = {\n",
    "            'image': image, \n",
    "            'category': torch.tensor(category, dtype=torch.long)\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), #resizes image to a square\n",
    "    transforms.ToTensor() #converts image to a PyTorch tensor\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset('training.csv', 'training_images', transform=transform)\n",
    "val_dataset = CustomDataset('validation.csv', 'training_images', transform=transform)\n",
    "test_dataset = CustomDataset('testing.csv', 'training_images', transform=transform)\n",
    "\n",
    "# DataLoader loads and manages data\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce49df6-1538-4beb-8a65-af9f9bbdd3e8",
   "metadata": {},
   "source": [
    "## Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4cdee3f-16b4-4ad3-bd33-d20991f41917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 0.21582384904225668, Validation Loss: 1.489869475364685\n",
      "Epoch 1/10, Training Loss: 0.3890514241324531, Validation Loss: 1.6607062816619873\n",
      "Epoch 1/10, Training Loss: 0.6084346108966403, Validation Loss: 1.601088285446167\n",
      "Epoch 1/10, Training Loss: 0.8482757541868422, Validation Loss: 1.4109405279159546\n",
      "Epoch 1/10, Training Loss: 1.0133864217334323, Validation Loss: 1.3991879224777222\n",
      "Epoch 1/10, Training Loss: 1.1772932079103258, Validation Loss: 1.405877947807312\n",
      "Epoch 1/10, Training Loss: 1.350628627671136, Validation Loss: 1.3928982615470886\n",
      "Epoch 1/10, Training Loss: 1.5054910315407648, Validation Loss: 1.3881806135177612\n",
      "Epoch 1/10, Training Loss: 1.6643512778811984, Validation Loss: 1.3890401124954224\n",
      "Epoch 2/10, Training Loss: 0.3883405791388618, Validation Loss: 1.3919861912727356\n",
      "Epoch 2/10, Training Loss: 0.54933676454756, Validation Loss: 1.4229751229286194\n",
      "Epoch 2/10, Training Loss: 0.7049454185697768, Validation Loss: 1.436991572380066\n",
      "Epoch 2/10, Training Loss: 0.8764743672476875, Validation Loss: 1.3990623354911804\n",
      "Epoch 2/10, Training Loss: 1.026146133740743, Validation Loss: 1.5330436825752258\n",
      "Epoch 2/10, Training Loss: 1.1962129142549303, Validation Loss: 1.4301912188529968\n",
      "Epoch 2/10, Training Loss: 1.3455509742101033, Validation Loss: 1.4577956199645996\n",
      "Epoch 2/10, Training Loss: 1.5050280623965793, Validation Loss: 1.4732486009597778\n",
      "Epoch 2/10, Training Loss: 1.607279982831743, Validation Loss: 1.7706784009933472\n",
      "Epoch 3/10, Training Loss: 0.08975422382354736, Validation Loss: 1.8421276211738586\n",
      "Epoch 3/10, Training Loss: 0.2949163383907742, Validation Loss: 1.7320646047592163\n",
      "Epoch 3/10, Training Loss: 0.547576559914483, Validation Loss: 1.4946616888046265\n",
      "Epoch 3/10, Training Loss: 0.6539944012959799, Validation Loss: 1.4385592937469482\n",
      "Epoch 3/10, Training Loss: 0.8361597590976291, Validation Loss: 1.4030352234840393\n",
      "Epoch 3/10, Training Loss: 0.9923790428373549, Validation Loss: 1.3925803303718567\n",
      "Epoch 3/10, Training Loss: 1.1479922268125746, Validation Loss: 1.3891552090644836\n",
      "Epoch 3/10, Training Loss: 1.3023816744486492, Validation Loss: 1.3886287212371826\n",
      "Epoch 3/10, Training Loss: 1.4524619976679485, Validation Loss: 1.3910463452339172\n",
      "Epoch 4/10, Training Loss: 0.372461822297838, Validation Loss: 1.4418761134147644\n",
      "Epoch 4/10, Training Loss: 0.5323259565565321, Validation Loss: 1.41086345911026\n",
      "Epoch 4/10, Training Loss: 0.7074274751875136, Validation Loss: 1.3884950876235962\n",
      "Epoch 4/10, Training Loss: 0.8551597462760078, Validation Loss: 1.3871169686317444\n",
      "Epoch 4/10, Training Loss: 1.0090458922915988, Validation Loss: 1.3866464495658875\n",
      "Epoch 4/10, Training Loss: 1.1614401870303683, Validation Loss: 1.3871937990188599\n",
      "Epoch 4/10, Training Loss: 1.3135141531626384, Validation Loss: 1.3881596326828003\n",
      "Epoch 4/10, Training Loss: 1.4727821350097656, Validation Loss: 1.3883045315742493\n",
      "Epoch 4/10, Training Loss: 1.6259591976801555, Validation Loss: 1.3887234926223755\n",
      "Epoch 5/10, Training Loss: 0.237047725253635, Validation Loss: 1.391849935054779\n",
      "Epoch 5/10, Training Loss: 0.3916255103217231, Validation Loss: 1.3920347094535828\n",
      "Epoch 5/10, Training Loss: 0.5542822943793403, Validation Loss: 1.3888784050941467\n",
      "Epoch 5/10, Training Loss: 0.7055277956856622, Validation Loss: 1.3878665566444397\n",
      "Epoch 5/10, Training Loss: 0.8573607073889838, Validation Loss: 1.3890575766563416\n",
      "Epoch 5/10, Training Loss: 1.0146996974945068, Validation Loss: 1.3902404308319092\n",
      "Epoch 5/10, Training Loss: 1.1646445857153997, Validation Loss: 1.3924963474273682\n",
      "Epoch 5/10, Training Loss: 1.3259265820185344, Validation Loss: 1.394071877002716\n",
      "Epoch 5/10, Training Loss: 1.4854865074157715, Validation Loss: 1.3934837579727173\n",
      "Epoch 6/10, Training Loss: 0.1289728217654758, Validation Loss: 1.3892455101013184\n",
      "Epoch 6/10, Training Loss: 0.28137364652421737, Validation Loss: 1.3893461227416992\n",
      "Epoch 6/10, Training Loss: 0.43316707346174455, Validation Loss: 1.389714539051056\n",
      "Epoch 6/10, Training Loss: 0.5877373483445909, Validation Loss: 1.389781653881073\n",
      "Epoch 6/10, Training Loss: 0.7398729456795586, Validation Loss: 1.3900009393692017\n",
      "Epoch 6/10, Training Loss: 0.8903418911827935, Validation Loss: 1.3908376097679138\n",
      "Epoch 6/10, Training Loss: 1.0423642661836412, Validation Loss: 1.3920507431030273\n",
      "Epoch 6/10, Training Loss: 1.1964486042658489, Validation Loss: 1.395823359489441\n",
      "Epoch 6/10, Training Loss: 1.3680186933941312, Validation Loss: 1.393882930278778\n",
      "Epoch 7/10, Training Loss: 0.20858758025699192, Validation Loss: 1.3903547525405884\n",
      "Epoch 7/10, Training Loss: 0.3628486394882202, Validation Loss: 1.389894962310791\n",
      "Epoch 7/10, Training Loss: 0.5222644673453437, Validation Loss: 1.3892468214035034\n",
      "Epoch 7/10, Training Loss: 0.6710556215710111, Validation Loss: 1.3893615007400513\n",
      "Epoch 7/10, Training Loss: 0.8240402539571127, Validation Loss: 1.3891106843948364\n",
      "Epoch 7/10, Training Loss: 0.9796639283498129, Validation Loss: 1.389083206653595\n",
      "Epoch 7/10, Training Loss: 1.1392054955164592, Validation Loss: 1.3887276649475098\n",
      "Epoch 7/10, Training Loss: 1.2891428073247273, Validation Loss: 1.388822078704834\n",
      "Epoch 7/10, Training Loss: 1.4484112527635362, Validation Loss: 1.3883454203605652\n",
      "Epoch 8/10, Training Loss: 0.18469338946872288, Validation Loss: 1.4467957019805908\n",
      "Epoch 8/10, Training Loss: 0.3337714539633857, Validation Loss: 12.740952491760254\n",
      "Epoch 8/10, Training Loss: 2.2481486664877997, Validation Loss: 1.3900825381278992\n",
      "Epoch 8/10, Training Loss: 2.3996224800745645, Validation Loss: 1.3900390267372131\n",
      "Epoch 8/10, Training Loss: 2.553987701733907, Validation Loss: 1.3909510374069214\n",
      "Epoch 8/10, Training Loss: 2.7016521427366467, Validation Loss: 1.408301591873169\n",
      "Epoch 8/10, Training Loss: 2.858172787560357, Validation Loss: 1.4756833910942078\n",
      "Epoch 8/10, Training Loss: 2.9991528458065457, Validation Loss: 2.3479843139648438\n",
      "Epoch 8/10, Training Loss: 3.3645805252922907, Validation Loss: 1.5110433101654053\n",
      "Epoch 9/10, Training Loss: 0.1308117707570394, Validation Loss: 1.47507244348526\n",
      "Epoch 9/10, Training Loss: 0.2649538384543525, Validation Loss: 1.564914047718048\n",
      "Epoch 9/10, Training Loss: 0.43776624732547337, Validation Loss: 1.5310543775558472\n",
      "Epoch 9/10, Training Loss: 0.5964971913231744, Validation Loss: 1.490708589553833\n",
      "Epoch 9/10, Training Loss: 0.8009829256269667, Validation Loss: 1.4048893451690674\n",
      "Epoch 9/10, Training Loss: 0.9578283627827963, Validation Loss: 1.3988997340202332\n",
      "Epoch 9/10, Training Loss: 1.1129091183344524, Validation Loss: 1.3869266510009766\n",
      "Epoch 9/10, Training Loss: 1.2696898645824857, Validation Loss: 1.387235939502716\n",
      "Epoch 9/10, Training Loss: 1.4230037795172796, Validation Loss: 1.3887730240821838\n",
      "Epoch 10/10, Training Loss: 0.05141400628619724, Validation Loss: 1.427054762840271\n",
      "Epoch 10/10, Training Loss: 0.20344516303804186, Validation Loss: 1.734983205795288\n",
      "Epoch 10/10, Training Loss: 0.298791507879893, Validation Loss: 2.5957001447677612\n",
      "Epoch 10/10, Training Loss: 0.7909819748666551, Validation Loss: 1.4117834568023682\n",
      "Epoch 10/10, Training Loss: 0.932127919461992, Validation Loss: 1.4459437131881714\n",
      "Epoch 10/10, Training Loss: 1.1010432971848383, Validation Loss: 1.5285654664039612\n",
      "Epoch 10/10, Training Loss: 1.225563559267256, Validation Loss: 23.738452911376953\n",
      "Epoch 10/10, Training Loss: 5.090786914030711, Validation Loss: 1.5074949860572815\n",
      "Epoch 10/10, Training Loss: 5.304400967227088, Validation Loss: 1.4429610967636108\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim # hosts optimization algorithms\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # loss function - calculate negative log likelihood\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # specifying Adam optimizer, learning rate, model parameters to update\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0 # initialized to 0 to track accumulated loss for that epoch\n",
    "    for batch in train_loader:\n",
    "        images = batch['image'].to(device)\n",
    "        categories = batch['category'].to(device)\n",
    "\n",
    "        optimizer.zero_grad() # reset gradients to 0 before each optimization step\n",
    "\n",
    "        pred_categories = model(images) # feeds input images through neural network model\n",
    "\n",
    "        loss = criterion(pred_categories, categories)\n",
    "        loss.backward() # computes loss gradient and performs backpropogation to investigate each parameter's impact on loss\n",
    "        optimizer.step() # update model parameters based on calculated gradients\n",
    "\n",
    "        running_train_loss += loss.item() # calculate loss for the entire epoch\n",
    "\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad(): # don't need to calculate gradient for validation and testing\n",
    "            for batch in val_loader:\n",
    "                images = batch['image'].to(device)\n",
    "                categories = batch['category'].to(device)\n",
    "                \n",
    "                pred_categories = model(images)\n",
    "                \n",
    "                loss = criterion(pred_categories, categories)\n",
    "                running_val_loss += loss.item()\n",
    "        \n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {running_train_loss/len(train_loader)}, Validation Loss: {running_val_loss/len(val_loader)}\"\n",
    "        )\n",
    "\n",
    "print(\"Training complete!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a358e4c7-4837-4faa-b7aa-b0b0d91b41ab",
   "metadata": {},
   "source": [
    "## Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0814d3af-95f3-4a41-945e-19aa187e406d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.2472459971904755, Test Accuracy: 20.00%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "running_test_loss = 0.0\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images = batch['image'].to(device)\n",
    "        categories = batch['category'].to(device)\n",
    "        \n",
    "        pred_categories = model(images)\n",
    "        \n",
    "        loss = criterion(pred_categories, categories)\n",
    "        running_test_loss += loss.item()\n",
    "        \n",
    "        _, predicted_labels = torch.max(pred_categories, 1) # discard the max value, only need its indices\n",
    "        correct_predictions += (predicted_labels == categories).sum().item() # looks at number of correct labels in the batch\n",
    "        total_predictions += categories.size(0)\n",
    "\n",
    "test_loss = running_test_loss / len(test_loader)\n",
    "accuracy = correct_predictions / total_predictions\n",
    "\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdf3538-10d1-43c2-b480-c9bd723c1786",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4159fad6-96de-4ac1-877d-8b455dee0485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "print(\"Model is saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
